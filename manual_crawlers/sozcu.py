"""
SÃ¶zcÃ¼ Manuel Crawler
--------------------
RSS'i Ã§alÄ±ÅŸmadÄ±ÄŸÄ± iÃ§in HTML parsing ile haberleri Ã§eker.
"""

import requests
from selectolax.parser import HTMLParser
from datetime import datetime
from urllib.parse import urljoin
import sys
import os

# Ana klasÃ¶rÃ¼ Python path'ine ekle
sys.path.insert(0, os.path.dirname(os.path.dirname(os.path.abspath(__file__))))

try:
    from config import REQUEST_TIMEOUT, USER_AGENT
except:
    REQUEST_TIMEOUT = 10
    USER_AGENT = "Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36"


def crawl_sozcu():
    """
    SÃ¶zcÃ¼ anasayfasÄ±ndan haberleri Ã§eker
    
    Returns:
        list: Haber listesi (dict'lerden oluÅŸan)
    """
    print("\n  ğŸ” https://www.sozcu.com.tr/")
    print("     â””â”€ HTML parsing ile Ã§ekiliyor...")
    
    base_url = "https://www.sozcu.com.tr"
    haberler = []
    
    try:
        # User-Agent ekleyerek istek at
        headers = {"User-Agent": USER_AGENT}
        
        response = requests.get(base_url, headers=headers, timeout=REQUEST_TIMEOUT)
        response.raise_for_status()
        
        # HTML'i parse et
        tree = HTMLParser(response.text)
        
        # TÃ¼m linkleri tara
        goruldu = set()  # Duplicate kontrolÃ¼ iÃ§in
        
        for link_element in tree.css("a"):
            href = link_element.attributes.get("href", "")
            
            # BoÅŸ link atla
            if not href:
                continue
            
            # Absolute URL yap
            if not href.startswith("http"):
                href = urljoin(base_url, href)
            
            # Sadece sozcu.com.tr domain'inden al
            if "sozcu.com.tr" not in href:
                continue
            
            # Duplicate kontrolÃ¼
            if href in goruldu:
                continue
            
            # Ana sayfa, kategori sayfalarÄ± vb. atla - sadece haber detay sayfalarÄ±
            # SÃ¶zcÃ¼'de haberler genelde sayÄ± ile biten URL'lerde
            if not any(char.isdigit() for char in href.split('/')[-1]):
                continue
            
            # BaÅŸlÄ±k al
            baslik = link_element.text(strip=True)
            
            # EÄŸer baÅŸlÄ±k link iÃ§inde yoksa, title attribute'dan al
            if not baslik:
                baslik = link_element.attributes.get("title", "")
            
            # Ã‡ok kÄ±sa baÅŸlÄ±klarÄ± atla
            if not baslik or len(baslik) < 10:
                continue
            
            # Haber objesi oluÅŸtur
            haber = {
                "baslik": baslik.strip(),
                "ozet": None,
                "tarih": datetime.now(),
                "kaynak": "sozcu",
                "url": href
            }
            
            haberler.append(haber)
            goruldu.add(href)
            
            # Limit
            if len(haberler) >= 50:
                break
        
        print(f"     â””â”€ âœ… {len(haberler)} haber bulundu")
        
    except requests.Timeout:
        print(f"     â””â”€ â±ï¸  Timeout!")
    except requests.RequestException as e:
        print(f"     â””â”€ âŒ HTTP hatasÄ±: {e}")
    except Exception as e:
        print(f"     â””â”€ âŒ Beklenmeyen hata: {e}")
    
    return haberler


if __name__ == "__main__":
    # Test iÃ§in
    haberler = crawl_sozcu()
    print(f"\nâœ… Test tamamlandÄ±. {len(haberler)} haber bulundu.")
    if haberler:
        print("\nÄ°lk 3 haber:")
        for i, haber in enumerate(haberler[:3], 1):
            print(f"\n{i}. BaÅŸlÄ±k: {haber['baslik']}")
            print(f"   URL: {haber['url']}")