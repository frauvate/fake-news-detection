"""
RSS Feed Crawler
-----------------
RSS destekli haber sitelerinden (SÃ¶zcÃ¼, BBC TÃ¼rkÃ§e, Sputnik, NTV) 
haberleri Ã§ekip MongoDB'ye kaydeder.

Ã–zellikler:
- robots.txt kontrolÃ¼
- Duplicate kontrolÃ¼ (aynÄ± haber 2 kez kaydedilmez)
- DetaylÄ± log tutma
- Hata yÃ¶netimi
"""

import feedparser
from pymongo import MongoClient
from datetime import datetime
from selectolax.parser import HTMLParser
import time
from urllib.robotparser import RobotFileParser
from urllib.parse import urlparse
import requests

from config import (
    MONGO_CONNECTION_STRING, 
    VERITABANI_ADI, 
    HABERLER_KOLEKSIYONU, 
    LOG_KOLEKSIYONU,
    REQUEST_TIMEOUT,
    REQUEST_DELAY,
    USER_AGENT
)

# RSS Feed KaynaklarÄ±
# Her site iÃ§in birden fazla kategori ekleyerek daha fazla haber toplanÄ±r
RSS_FEEDS = {
    "sozcu": [
        "https://www.sozcu.com.tr/feed/",
        "https://www.sozcu.com.tr/kategori/gundem/feed/",
        "https://www.sozcu.com.tr/kategori/ekonomi/feed/",
        "https://www.sozcu.com.tr/kategori/dunya/feed/",
        "https://www.sozcu.com.tr/kategori/spor/feed/",
    ],
    "bbc": [
        "https://feeds.bbci.co.uk/turkce/rss.xml",
    ],
    "sputnik": [
        "https://tr.sputniknews.com/export/rss2/archive/index.xml",
    ],
    "ntv": [
        "https://www.ntv.com.tr/gundem.rss",
        "https://www.ntv.com.tr/ekonomi.rss",
        "https://www.ntv.com.tr/dunya.rss",
        "https://www.ntv.com.tr/turkiye.rss",
        "https://www.ntv.com.tr/teknoloji.rss",
        "https://www.ntv.com.tr/egitim.rss",
        "https://www.ntv.com.tr/saglik.rss",
        "https://www.ntv.com.tr/yasam.rss",
    ]
}


def html_temizle(html_metin):
    """
    HTML etiketlerini temizleyip saf metni dÃ¶ndÃ¼rÃ¼r
    
    Args:
        html_metin (str): HTML iÃ§eren metin
    
    Returns:
        str: TemizlenmiÅŸ metin veya None
    """
    if not html_metin:
        return None
    try:
        parser = HTMLParser(html_metin)
        temiz_metin = parser.text().strip()
        return temiz_metin if temiz_metin else None
    except Exception as e:
        return None


def robots_txt_kontrol(url):
    """
    Verilen URL'nin robots.txt'ine gÃ¶re eriÅŸilebilir olup olmadÄ±ÄŸÄ±nÄ± kontrol eder
    
    Args:
        url (str): Kontrol edilecek URL
    
    Returns:
        bool: EriÅŸim izni varsa True, yoksa False
    """
    try:
        parsed = urlparse(url)
        base_url = f"{parsed.scheme}://{parsed.netloc}"
        
        rp = RobotFileParser()
        rp.set_url(f"{base_url}/robots.txt")
        rp.read()
        
        # User-agent olarak '*' kullanÄ±yoruz (genel bot)
        izin = rp.can_fetch("*", url)
        return izin
    except Exception as e:
        # Hata durumunda gÃ¼venli tarafta kal, izin var say
        print(f"    âš ï¸  robots.txt kontrol hatasÄ±: {e}")
        return True


def tarih_parse(entry):
    """
    RSS entry'sinden tarihi parse eder
    
    Args:
        entry: feedparser entry objesi
    
    Returns:
        datetime: Parse edilmiÅŸ tarih veya ÅŸimdiki zaman
    """
    # Ã–nce published, sonra updated alanlarÄ±nÄ± kontrol et
    tarih_str = getattr(entry, "published", None) or getattr(entry, "updated", None)
    
    if tarih_str:
        try:
            # published_parsed veya updated_parsed varsa kullan
            if hasattr(entry, "published_parsed") and entry.published_parsed:
                return datetime(*entry.published_parsed[:6])
            elif hasattr(entry, "updated_parsed") and entry.updated_parsed:
                return datetime(*entry.updated_parsed[:6])
        except Exception as e:
            pass
    
    # Parse edilemezse ÅŸimdiki zamanÄ± kullan
    return datetime.now()


def rss_cek(rss_url, kaynak_adi, collection, log_collection):
    """
    Belirtilen RSS feed'den haberleri Ã§eker ve MongoDB'ye kaydeder
    
    Args:
        rss_url (str): RSS feed URL'i
        kaynak_adi (str): Kaynak adÄ± (Ã¶rn: "sozcu")
        collection: MongoDB articles koleksiyonu
        log_collection: MongoDB logs koleksiyonu
    
    Returns:
        dict: Log bilgileri
    """
    baslangic_zamani = time.time()
    
    # Log objesi oluÅŸtur
    log = {
        "cekim_zamani": datetime.now(),
        "kaynak": kaynak_adi,
        "endpoint": rss_url,
        "basarili": False,
        "durum": "fail",  # basarili/fail/timeout
        "hata_mesaji": None,
        "cekilen_haber_sayisi": 0,
        "sure_saniye": None
    }
    
    try:
        print(f"\n  ğŸ” {rss_url}")
        print(f"     â””â”€ Ã‡ekiliyor...")
        
        # robots.txt kontrolÃ¼
        if not robots_txt_kontrol(rss_url):
            log["hata_mesaji"] = "robots.txt tarafÄ±ndan engellenmiÅŸ"
            log["durum"] = "fail"
            print(f"     â””â”€ âŒ robots.txt izin vermiyor!")
            return log
        
        # RSS feed'i parse et (timeout ile)
        try:
            # feedparser timeout desteklemiyor, requests ile Ã¶nce Ã§ekelim
            headers = {"User-Agent": USER_AGENT}
            response = requests.get(rss_url, headers=headers, timeout=REQUEST_TIMEOUT)
            feed = feedparser.parse(response.content)
        except requests.Timeout:
            log["hata_mesaji"] = "Timeout - BaÄŸlantÄ± zaman aÅŸÄ±mÄ±na uÄŸradÄ±"
            log["durum"] = "timeout"
            print(f"     â””â”€ â±ï¸  Timeout!")
            return log
        except Exception as e:
            log["hata_mesaji"] = f"Ä°stek hatasÄ±: {str(e)}"
            log["durum"] = "fail"
            print(f"     â””â”€ âŒ Ä°stek hatasÄ±!")
            return log
        
        # Feed'de hata var mÄ± kontrol et
        if feed.bozo:
            log["hata_mesaji"] = f"RSS parse hatasÄ±: {getattr(feed, 'bozo_exception', 'Bilinmeyen hata')}"
            log["durum"] = "fail"
            print(f"     â””â”€ âŒ RSS parse hatasÄ±!")
            return log
        
        # Haber yoksa
        if not feed.entries:
            log["hata_mesaji"] = "RSS feed'inde haber bulunamadÄ±"
            log["durum"] = "fail"
            print(f"     â””â”€ âš ï¸  Haber bulunamadÄ±!")
            return log
        
        # Haberleri iÅŸle
        eklenen_sayi = 0
        taranan_sayi = len(feed.entries)
        
        for entry in feed.entries:
            # Gerekli alanlarÄ± al
            baslik = getattr(entry, "title", None)
            link = getattr(entry, "link", None)
            ozet_html = getattr(entry, "summary", None) or getattr(entry, "description", None)
            ozet = html_temizle(ozet_html)
            tarih = tarih_parse(entry)
            
            # Link yoksa atla
            if not link:
                continue
            
            # BaÅŸlÄ±k yoksa atla
            if not baslik or len(baslik.strip()) < 5:
                continue
            
            # Haber objesi oluÅŸtur
            haber = {
                "baslik": baslik.strip(),
                "ozet": ozet,
                "tarih": tarih,
                "kaynak": kaynak_adi,
                "url": link,
                "eklenme_zamani": datetime.now()
            }
            
            # AynÄ± URL zaten varsa atla (duplicate kontrolÃ¼)
            if collection.find_one({"url": link}):
                continue
            
            # MongoDB'ye kaydet
            try:
                collection.insert_one(haber)
                eklenen_sayi += 1
            except Exception as e:
                # Unique index hatasÄ± veya baÅŸka bir hata
                pass
        
        # BaÅŸarÄ±lÄ±
        log["basarili"] = True
        log["durum"] = "basarili"
        log["cekilen_haber_sayisi"] = eklenen_sayi
        print(f"     â””â”€ âœ… {eklenen_sayi}/{taranan_sayi} yeni haber eklendi")
        
    except Exception as e:
        log["hata_mesaji"] = str(e)
        log["durum"] = "fail"
        print(f"     â””â”€ âŒ Beklenmeyen hata: {e}")
    
    finally:
        # SÃ¼reyi hesapla
        log["sure_saniye"] = round(time.time() - baslangic_zamani, 2)
        
        # Log'u kaydet
        try:
            log_collection.insert_one(log)
        except Exception as e:
            print(f"     â””â”€ âš ï¸  Log kaydedilemedi: {e}")
    
    return log


def rss_crawler_calistir():
    """
    Ana fonksiyon - TÃ¼m RSS kaynaklarÄ±nÄ± Ã§eker
    
    Returns:
        dict: Ã–zet istatistikler
    """
    print("=" * 80)
    print("ğŸŒ RSS CRAWLER BAÅLATILIYOR")
    print("=" * 80)
    
    # MongoDB'ye baÄŸlan
    try:
        client = MongoClient(MONGO_CONNECTION_STRING)
        db = client[VERITABANI_ADI]
        articles = db[HABERLER_KOLEKSIYONU]
        logs = db[LOG_KOLEKSIYONU]
        
        print(f"\nâœ… MongoDB baÄŸlantÄ±sÄ± baÅŸarÄ±lÄ±!")
        print(f"ğŸ“ VeritabanÄ±: {VERITABANI_ADI}")
        
        # Ä°ndeksler oluÅŸtur (ilk Ã§alÄ±ÅŸtÄ±rmada)
        try:
            articles.create_index("url", unique=True)
            articles.create_index("kaynak")
            articles.create_index("tarih")
            print("âœ… VeritabanÄ± indeksleri hazÄ±r")
        except:
            pass  # Zaten varsa hata vermesin
        
        print(f"ğŸ“Š Mevcut haber sayÄ±sÄ±: {articles.count_documents({})}")
        
    except Exception as e:
        print(f"âŒ MongoDB baÄŸlantÄ± hatasÄ±: {e}")
        return None
    
    # Toplam istatistikler
    toplam_cekilen = 0
    toplam_basarili = 0
    toplam_basarisiz = 0
    toplam_timeout = 0
    
    # Her RSS feed iÃ§in
    for kaynak_adi, feed_listesi in RSS_FEEDS.items():
        print(f"\n{'=' * 80}")
        print(f"ğŸ“¡ {kaynak_adi.upper()} ({len(feed_listesi)} feed)")
        print(f"{'=' * 80}")
        
        for rss_url in feed_listesi:
            # RSS'i Ã§ek
            log = rss_cek(rss_url, kaynak_adi, articles, logs)
            
            # Ä°statistikleri gÃ¼ncelle
            if log["basarili"]:
                toplam_basarili += 1
                toplam_cekilen += log["cekilen_haber_sayisi"]
            elif log["durum"] == "timeout":
                toplam_timeout += 1
            else:
                toplam_basarisiz += 1
            
            # Rate limiting (siteye aÅŸÄ±rÄ± yÃ¼k bindirmemek iÃ§in bekle)
            time.sleep(REQUEST_DELAY)
    
    # Ã–zet rapor
    print(f"\n{'=' * 80}")
    print("ğŸ“Š RSS CRAWLER Ã–ZET RAPOR")
    print(f"{'=' * 80}")
    print(f"âœ… BaÅŸarÄ±lÄ± Ã§ekimler:    {toplam_basarili}")
    print(f"âŒ BaÅŸarÄ±sÄ±z Ã§ekimler:   {toplam_basarisiz}")
    print(f"â±ï¸  Timeout:              {toplam_timeout}")
    print(f"ğŸ“° Bu Ã§alÄ±ÅŸmada eklenen: {toplam_cekilen} haber")
    print(f"ğŸ“ Toplam haber sayÄ±sÄ±:  {articles.count_documents({})} haber")
    print(f"{'=' * 80}")
    
    return {
        "basarili": toplam_basarili,
        "basarisiz": toplam_basarisiz,
        "timeout": toplam_timeout,
        "eklenen": toplam_cekilen,
        "toplam": articles.count_documents({})
    }


if __name__ == "__main__":
    rss_crawler_calistir()
    print("\nâœ¨ RSS Crawler tamamlandÄ±!\n")
